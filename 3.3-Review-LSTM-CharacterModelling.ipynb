{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png\" width = 300, align = \"center\"></a>\n",
    "\n",
    "\n",
    "# <center> Text generation using RNN/LSTM (Character-level)</center>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 3><strong>In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network</strong></font>\n",
    "<br>    \n",
    "- <a href=\"#intro\">Introduction</a>\n",
    "<br>\n",
    "- <p><a href=\"#arch\">Architectures</a></p>\n",
    "    - <a href=\"#lstm\">Long Short-Term Memory Model (LSTM)</a>\n",
    "\n",
    "- <p><a href=\"#build\">Building a LSTM with TensorFlow</a></p>\n",
    "</div>\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the requiered libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "The following cell is a class that help to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "#### Batch, number_of_batch, batch_size and seq_length\n",
    "what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  \n",
    "\n",
    "Lets assume the input is this sentence: '__here is an example__'. Then:\n",
    "- txt_length = 18  \n",
    "- seq_length = 3  \n",
    "- batch_size = 2  \n",
    "- number_of_batchs = 18/3*2 = 3\n",
    "- batch = array (['h','e','r'],['e',' ','i'])\n",
    "- sample Seq = 'her'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now, lets look at a real dataset, with real parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 60  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 125 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the input file, and print a part of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-28 19:53:08 URL:https://public.boxcloud.com/d/1/b1!RWxv2aviqj0NzNQrleogc6tFswy8wAvbRIUU2ai0S1pEbPUN96d6ryti7ZTfLpU-tyZlNOWTJrZLf2uJSuzGilpTKrbhWqz6rqfp6QegqX2_PJXoy_Xd2uBZ9NGTmP4xWExb1tcCIwrgKlYMhFMLrTSKBVtMPeYFlqsEyb4BOJ03sgwm7iuY1Ouo1FxWcitHXW1mLN_ZSUa7OLPAO3vfb5l93uJMUDY5burtd-2P-90wxseCNAMlxdgkk2Y60Mpjk0RNgNjYYUF8Op3i30WPxvgz2oqa-0nOCZGDFdu60F6r6t2zsziFUzji8D2F4yAfsy7bjE0SC5CEj8ZzN8WFNqVSRYFoe3ITCVqejL864CzYBVtnuXeozL5481ElkEsL7oy3mAjAWeGgOrnqxe7BHabguW_o7_DnPJs39cf34LRR9m757DTLuQBaZzhHU0V6IK24-MLy2tYFlnUv6iFMQvPIQQD4XMAiJoFUkERKqPQXXQ6vV9w1Bq-yVv4q40-Ijx5JpfEZrr3zZsG5FO1n_GFix7YdzpqNWzXc9Km3kzQkOqlP_Skyan0sdufsN15JqZQg4mrT_6_a1HgjOgFs7r_emHBHByFFsTlvUgtggiqFLMxAgVGAspWYCeUpOZaCuFgdqAv-2oWwfvsKNJjmzZIw6VihRy9xtQ2hELvkisL0yqoLYlNA1Xe5AwYE4Jjd1gTuIyHpS2Ijm2yUigx4vNVyTCIuZmWG6otQqux6K0wr3RMeAULcbQUfYcpor-eILyLRVU9fgm4Id4xfm_-21t_13PujuCZPvRWE_nYv10aOD8Rmp8fsHJSQJn2JQMcFf3dLRGSvJkkIj9PWAz9on3BnU8LZd6i4zLM1W0S1F8Jq3xZc8Zhk0OXi5eBPn190fCGZtXCaaNknal3iA3k6hCzBXHVkBZt87aidaatC481Jb6kg0AmZDcggiG5GSL4gXaFFIWFHP0gh3Cc1DiSJQ-ZNQhWQyFJnTJll5OupbHQmMg9tdTOpwNpW9XEdNerzS_Mp8HB0bloCdUtkHA9eNkfFv9UD_aJfrjtYteUXHMO1jVp5JSn-VtYsvJfUg6eZD00-33x6Oa00umlahnYGhR4iFD341gMqTS-QlT803e7Xe85rMZFhdFBNdQ_JXknn0U6KED3oFYrHiwoPlbT1qikuHUC8DvClH1X7rnEzbk75XqABw0krqTrvIdOubE1X9R286u8KJ-fvg17qpW-uycYJDgO4GySh7GjAY54QCkC_p0WaSUa3qUN-oLlb-Wgyg4z6pCUcL_vs0cJpA-wS51MLD1SD7Pq61thPPb3SQxiAZh3gqm4-qm-x08oNRpf_L_kcjvRPoj0LmDDtAYLUwwoFv1alP978Aa4RXfkO0AA36PRvVHGj8vk./download [1115393/1115393] -> \"input.txt\" [1]\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print (read_data[0:100])\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file\n",
      "vocabulary size: 65\n",
      "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [19  4 14 ... 14  9 20]\n",
      " [ 8 20 10 ...  8 10 18]\n",
      " ...\n",
      " [21  2  0 ...  0 21  0]\n",
      " [ 9  7  7 ...  0  2  3]\n",
      " [ 3  7  0 ...  5  9 23]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print (\"vocabulary size:\" ,data_loader.vocab_size)\n",
    "print (\"Characters:\" ,data_loader.chars)\n",
    "print (\"vocab number of 'F':\",data_loader.vocab['F'])\n",
    "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =60, seq_length=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, __y__ is the next character for each character in __x__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 4, 14, 22, ...,  9, 20,  5],\n",
       "       [20, 10, 29, ..., 10, 18,  4],\n",
       "       ...,\n",
       "       [ 2,  0,  6, ..., 21,  0,  6],\n",
       "       [ 7,  7,  4, ...,  2,  3,  0],\n",
       "       [ 7,  0, 33, ...,  9, 23,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Architecture\n",
    "Each LSTM cell has 5 parts:\n",
    "1. Input\n",
    "2. prv_state\n",
    "3. prv_output\n",
    "4. new_state\n",
    "5. new_output\n",
    "\n",
    "\n",
    "- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.\n",
    "- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size\n",
    "- An LSTM keeps two pieces of information as it propagates through time: \n",
    "    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n",
    "    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n",
    "\n",
    "\n",
    "#### num_layers = 2 \n",
    "- number of layers in the RNN, is defined by num_layers\n",
    "- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining stacked RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BasicRNNCell__ is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__state__ varibale keeps output and new_state of the LSTM, so it is a touple of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and target data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size\n",
    "representing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size ? 60x128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the value of the input_data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notice:__ The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the __embedding__, __em__, and __inputs__ variabbles:\n",
    "\n",
    "Embedding variable is initialized with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05103476,  0.09119304, -0.16861361, ..., -0.11314133,\n",
       "        -0.09372239,  0.09327243],\n",
       "       [ 0.02836111, -0.03340243,  0.12000407, ..., -0.14335835,\n",
       "         0.0881425 , -0.16392979],\n",
       "       [-0.12261036, -0.12018283,  0.12868373, ..., -0.07356536,\n",
       "         0.11572911, -0.02998717],\n",
       "       ...,\n",
       "       [ 0.15505038, -0.15356582,  0.15204622, ...,  0.14632095,\n",
       "        -0.10453272, -0.15277177],\n",
       "       [-0.16890594, -0.00896564,  0.01666404, ...,  0.04995041,\n",
       "         0.17621656,  0.15169086],\n",
       "       [ 0.04797241,  0.01896253, -0.07371249, ...,  0.16116752,\n",
       "         0.0954928 ,  0.02045549]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "#print embedding.shape\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.15393525,  0.1458384 , -0.1450935 , ..., -0.11883464,\n",
       "        -0.11272894, -0.03246666],\n",
       "       [-0.15371896, -0.00240661, -0.14233491, ...,  0.05160731,\n",
       "        -0.13522997,  0.07269035],\n",
       "       [-0.07207496,  0.15266494,  0.10014282, ...,  0.09443988,\n",
       "         0.08336981, -0.00992846],\n",
       "       ...,\n",
       "       [ 0.02836111, -0.03340243,  0.12000407, ..., -0.14335835,\n",
       "         0.0881425 , -0.16392979],\n",
       "       [ 0.00206093, -0.10127448, -0.09263828, ...,  0.11739196,\n",
       "         0.14007308,  0.00129694],\n",
       "       [-0.07207496,  0.15266494,  0.10014282, ...,  0.09443988,\n",
       "         0.08336981, -0.00992846]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print (emp.shape)\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider each sequence as a sentence of length 50 characters, then, the first item in __inputs__ is a [60x128] vector which represents the first characters of 60 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding a batch of 50 sequence to a RNN:\n",
    "\n",
    "The feeding process for iputs is as following:\n",
    "\n",
    "- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  \n",
    "- Step 2:  second character of each of the 50 sentences is input in parallel. \n",
    "- Step n: nth character of each of the 50 sentences is input in parallel.  \n",
    "\n",
    "The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5393525e-01,  1.4583839e-01, -1.4509350e-01, ...,\n",
       "        -1.1883464e-01, -1.1272894e-01, -3.2466665e-02],\n",
       "       [-1.5007208e-01,  1.5240248e-01,  1.6315773e-02, ...,\n",
       "         9.7902760e-02, -4.8448145e-04,  9.2250630e-02],\n",
       "       [ 4.0985316e-02,  1.5679027e-01,  8.5850373e-02, ...,\n",
       "         1.5393718e-01, -1.0053307e-02,  1.2789614e-01],\n",
       "       ...,\n",
       "       [ 1.4570355e-04, -4.6657920e-03, -1.3013701e-01, ...,\n",
       "         1.4593635e-01,  1.7039962e-01, -7.0761755e-02],\n",
       "       [-1.5371896e-01, -2.4066120e-03, -1.4233491e-01, ...,\n",
       "         5.1607311e-02, -1.3522997e-01,  7.2690353e-02],\n",
       "       [ 9.8114178e-02,  7.1897954e-03,  1.2426932e-01, ...,\n",
       "        -2.9110909e-04,  1.4807846e-01,  6.9001049e-02]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding the RNN with one batch, we can check the new output and new state of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output of network after feeding it with first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03699811, -0.16493021, -0.05564139, ...,  0.05714142,\n",
       "         0.08963489, -0.00993286],\n",
       "       [-0.0202904 ,  0.16614689, -0.06141068, ..., -0.06137393,\n",
       "        -0.08054513, -0.04118301],\n",
       "       [-0.0665277 , -0.00337516,  0.06707481, ...,  0.12197328,\n",
       "         0.06573988,  0.00069579],\n",
       "       ...,\n",
       "       [ 0.08524837,  0.08629622,  0.03398531, ...,  0.03695766,\n",
       "         0.10433447,  0.00783118],\n",
       "       [-0.03561621,  0.1506356 , -0.03034126, ..., -0.04504211,\n",
       "         0.04474264,  0.05281726],\n",
       "       [ 0.0646155 ,  0.007989  , -0.09816696, ..., -0.0741063 ,\n",
       "         0.0092329 ,  0.02385558]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was explained, __outputs__ variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The __softmax_w__ shape is [rnn_size, vocab_size],whihc is [128x65] in our case. Threfore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the __softmax(output * softmax_w + softmax_b)__ for this purpose. The shape of the matrixis would be:\n",
    "\n",
    "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the probablity of the next chracter in all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01726429, 0.01584125, 0.01076107, ..., 0.01860455, 0.01549697,\n",
       "        0.01159345],\n",
       "       [0.01578731, 0.01652722, 0.01594204, ..., 0.01984924, 0.01744928,\n",
       "        0.01163671],\n",
       "       [0.02155556, 0.01695372, 0.0131163 , ..., 0.01551264, 0.01522437,\n",
       "        0.01320129],\n",
       "       ...,\n",
       "       [0.01622067, 0.01739148, 0.01136027, ..., 0.02569882, 0.0138576 ,\n",
       "        0.01092899],\n",
       "       [0.02326459, 0.01701407, 0.00943875, ..., 0.01505913, 0.01363023,\n",
       "        0.01735632],\n",
       "       [0.0128851 , 0.01249819, 0.01587404, ..., 0.0204988 , 0.01255151,\n",
       "        0.00955605]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are in the position to calculate the cost of training with __loss function__, and keep feedng the network to learn it. But, the question is: what the LSTM networks learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together\n",
    "Now, let's put all of parts together in a class, and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            print(\">> sample mode:\")\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "        # model.cell.state_size is (128, 128)\n",
    "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "        # Initial state of the LSTM memory.\n",
    "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "        with tf.variable_scope('rnnlm_class1'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "                #inputs = tf.split(em, seq_length, 1)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        # The value of state is updated after processing each batch of chars.\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the LSTM object\n",
    "Now we create a LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    model = LSTMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train usinng LSTMModel class\n",
    "We can train our model through feeding batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/46375 (epoch 0), train_loss = 1.921, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The hame of elop thimen?\n",
      "\n",
      "GRUMESBBAY:\n",
      "I will prother:\n",
      "\n",
      "----------------------------------\n",
      "741/46375 (epoch 1), train_loss = 1.756, time/batch = 0.089\n",
      ">> sample mode:\n",
      "The do more himst headt you to good uswarn: 'tis grace\n",
      "----------------------------------\n",
      "1112/46375 (epoch 2), train_loss = 1.687, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The ont being so foom from lide me prow but thare our \n",
      "----------------------------------\n",
      "1483/46375 (epoch 3), train_loss = 1.639, time/batch = 0.064\n",
      ">> sample mode:\n",
      "The mot. I that how do desemporing basticing Righory m\n",
      "----------------------------------\n",
      "1854/46375 (epoch 4), train_loss = 1.608, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The tranies,\n",
      "That just thick, and no reath, angreat,\n",
      "T\n",
      "----------------------------------\n",
      "2225/46375 (epoch 5), train_loss = 1.588, time/batch = 0.064\n",
      ">> sample mode:\n",
      "The to recuel'd; sirry forbear lose of all the restram\n",
      "----------------------------------\n",
      "2596/46375 (epoch 6), train_loss = 1.572, time/batch = 0.071\n",
      ">> sample mode:\n",
      "The the Caming of land. Which be do.\n",
      "\n",
      "DUCHESS:\n",
      "Vinnal \n",
      "----------------------------------\n",
      "2967/46375 (epoch 7), train_loss = 1.560, time/batch = 0.067\n",
      ">> sample mode:\n",
      "The shalk before to fair waked and fight,\n",
      "Frive for't,\n",
      "----------------------------------\n",
      "3338/46375 (epoch 8), train_loss = 1.550, time/batch = 0.056\n",
      ">> sample mode:\n",
      "The with his\n",
      "faccrouse with should\n",
      "they see, both of h\n",
      "----------------------------------\n",
      "3709/46375 (epoch 9), train_loss = 1.540, time/batch = 0.066\n",
      ">> sample mode:\n",
      "The belaster?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Turt doth much,\n",
      "One comfock\n",
      "----------------------------------\n",
      "4080/46375 (epoch 10), train_loss = 1.530, time/batch = 0.079\n",
      ">> sample mode:\n",
      "The Exquitates, prey slain,\n",
      "Come she will be sevoral\n",
      "S\n",
      "----------------------------------\n",
      "4451/46375 (epoch 11), train_loss = 1.521, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The crows. I am, a the time name are deserved! and the\n",
      "----------------------------------\n",
      "4822/46375 (epoch 12), train_loss = 1.515, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The langer'd, and ambaling,' God us are, my lord,\n",
      "My f\n",
      "----------------------------------\n",
      "5193/46375 (epoch 13), train_loss = 1.510, time/batch = 0.093\n",
      ">> sample mode:\n",
      "The was unto ponkered, and go with wisted but come, er\n",
      "----------------------------------\n",
      "5564/46375 (epoch 14), train_loss = 1.505, time/batch = 0.071\n",
      ">> sample mode:\n",
      "The wars of some a three how Harry.\n",
      "\n",
      "RICHARD:\n",
      "Nay; no \n",
      "----------------------------------\n",
      "5935/46375 (epoch 15), train_loss = 1.502, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The that dare viefing\n",
      "With never soul.\n",
      "\n",
      "DUCHESS OFIZEL\n",
      "----------------------------------\n",
      "6306/46375 (epoch 16), train_loss = 1.498, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The only, your his\n",
      "gind. Lond, be contagh.\n",
      "But by heav\n",
      "----------------------------------\n",
      "6677/46375 (epoch 17), train_loss = 1.495, time/batch = 0.079\n",
      ">> sample mode:\n",
      "The shiple:\n",
      "Duspiss needs to his great the nepign! Or \n",
      "----------------------------------\n",
      "7048/46375 (epoch 18), train_loss = 1.491, time/batch = 0.185\n",
      ">> sample mode:\n",
      "The well to sue, the neights,\n",
      "But will, I have tur que\n",
      "----------------------------------\n",
      "7419/46375 (epoch 19), train_loss = 1.488, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The call;\n",
      "If?\n",
      "\n",
      "MISTRESS:\n",
      "Nay, my, to my court out in t\n",
      "----------------------------------\n",
      "7790/46375 (epoch 20), train_loss = 1.486, time/batch = 0.079\n",
      ">> sample mode:\n",
      "The bristice.\n",
      "\n",
      "PETRUCHIO:\n",
      "Pray it are task gues court \n",
      "----------------------------------\n",
      "8161/46375 (epoch 21), train_loss = 1.483, time/batch = 0.122\n",
      ">> sample mode:\n",
      "The alay.\n",
      "\n",
      "DORSARTIS:\n",
      "\n",
      "AUTOLYCUS:\n",
      "Yes, go be condress\n",
      "\n",
      "----------------------------------\n",
      "8532/46375 (epoch 22), train_loss = 1.481, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The worship in Angele; and he is:\n",
      "Net my bosner\n",
      "As her\n",
      "----------------------------------\n",
      "8903/46375 (epoch 23), train_loss = 1.479, time/batch = 0.056\n",
      ">> sample mode:\n",
      "The haunt, nor all of Lucentio.\n",
      "\n",
      "NORFOLK:\n",
      "Bristy with \n",
      "----------------------------------\n",
      "9274/46375 (epoch 24), train_loss = 1.477, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The sinn mine ressial were.\n",
      "\n",
      "SLY ANTIGONU:\n",
      "Confissiary\n",
      "----------------------------------\n",
      "9645/46375 (epoch 25), train_loss = 1.476, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The England's by what you even in this, that up jealti\n",
      "----------------------------------\n",
      "10016/46375 (epoch 26), train_loss = 1.474, time/batch = 0.085\n",
      ">> sample mode:\n",
      "The here.\n",
      "Come,\n",
      "Thought your good, pardon Cackury ther\n",
      "----------------------------------\n",
      "10387/46375 (epoch 27), train_loss = 1.472, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The if the gues!\n",
      "'Tis now?\n",
      "\n",
      "First O thicknongs, and ma\n",
      "----------------------------------\n",
      "10758/46375 (epoch 28), train_loss = 1.471, time/batch = 0.076\n",
      ">> sample mode:\n",
      "The ended magiin why grought the hour.\n",
      "\n",
      "HORTENSIO:\n",
      "How\n",
      "----------------------------------\n",
      "11129/46375 (epoch 29), train_loss = 1.469, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The her thine.\n",
      "\n",
      "JOgnd:\n",
      "Your hourer I'll Aars' time bei\n",
      "----------------------------------\n",
      "11500/46375 (epoch 30), train_loss = 1.468, time/batch = 0.055\n",
      ">> sample mode:\n",
      "The Autcher.\n",
      "Come, afteen of greet her! will endueady \n",
      "----------------------------------\n",
      "11871/46375 (epoch 31), train_loss = 1.466, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The world;\n",
      "Life this, I am not place?\n",
      "\n",
      "BRUTUS:\n",
      "Turn aw\n",
      "----------------------------------\n",
      "12242/46375 (epoch 32), train_loss = 1.465, time/batch = 0.067\n",
      ">> sample mode:\n",
      "The with her.\n",
      "It our life todlaffer and thy welcimman \n",
      "----------------------------------\n",
      "12613/46375 (epoch 33), train_loss = 1.463, time/batch = 0.067\n",
      ">> sample mode:\n",
      "The while!\n",
      "\n",
      "LUCENTIO:\n",
      "But knould they stiar of suppitt\n",
      "----------------------------------\n",
      "12984/46375 (epoch 34), train_loss = 1.462, time/batch = 0.067\n",
      ">> sample mode:\n",
      "The contest to commond shall clous well\n",
      "make my garmen\n",
      "----------------------------------\n",
      "13355/46375 (epoch 35), train_loss = 1.461, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The very queening frish by her:\n",
      "If we shall see ashies\n",
      "----------------------------------\n",
      "13726/46375 (epoch 36), train_loss = 1.459, time/batch = 0.074\n",
      ">> sample mode:\n",
      "The to Camest it, busine; and the set lost, that fight\n",
      "----------------------------------\n",
      "14097/46375 (epoch 37), train_loss = 1.458, time/batch = 0.064\n",
      ">> sample mode:\n",
      "The gere the minister glahour'd lord?\n",
      "\n",
      "LADY BUUTHUCHAS\n",
      "----------------------------------\n",
      "14468/46375 (epoch 38), train_loss = 1.457, time/batch = 0.082\n",
      ">> sample mode:\n",
      "The most time. A resenged upon thy head:\n",
      "Believe me, g\n",
      "----------------------------------\n",
      "14839/46375 (epoch 39), train_loss = 1.456, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The his arm, tents but peace, and think now:\n",
      "I\n",
      "would h\n",
      "----------------------------------\n",
      "15210/46375 (epoch 40), train_loss = 1.455, time/batch = 0.075\n",
      ">> sample mode:\n",
      "The land, give me change that you ports at this minds\n",
      "\n",
      "----------------------------------\n",
      "15581/46375 (epoch 41), train_loss = 1.454, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The joinomed\n",
      "lake,\n",
      "as fuck, peed?\n",
      "Yourst thou spuriein\n",
      "----------------------------------\n",
      "15952/46375 (epoch 42), train_loss = 1.453, time/batch = 0.078\n",
      ">> sample mode:\n",
      "The he hate, my lords,\n",
      "Adder, and noble poor can--ixfo\n",
      "----------------------------------\n",
      "16323/46375 (epoch 43), train_loss = 1.452, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The her plaudess by my such out of anfolding strumpets\n",
      "----------------------------------\n",
      "16694/46375 (epoch 44), train_loss = 1.451, time/batch = 0.104\n",
      ">> sample mode:\n",
      "The master:\n",
      "Too bound nail'd proyour lect with shoring\n",
      "----------------------------------\n",
      "17065/46375 (epoch 45), train_loss = 1.451, time/batch = 0.053\n",
      ">> sample mode:\n",
      "The master had with innocented on gall not have was su\n",
      "----------------------------------\n",
      "17436/46375 (epoch 46), train_loss = 1.450, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The where than I, sound that\n",
      "We have warvent conger:\n",
      "I\n",
      "----------------------------------\n",
      "17807/46375 (epoch 47), train_loss = 1.449, time/batch = 0.066\n",
      ">> sample mode:\n",
      "The are child, proker, drist.\n",
      "I was of the seass?\n",
      "\n",
      "Fir\n",
      "----------------------------------\n",
      "18178/46375 (epoch 48), train_loss = 1.448, time/batch = 0.091\n",
      ">> sample mode:\n",
      "The vice\n",
      "The one the truth; but no mount; I slay upon \n",
      "----------------------------------\n",
      "18549/46375 (epoch 49), train_loss = 1.447, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The conds'\n",
      "Should calman bractly couchar if these sudg\n",
      "----------------------------------\n",
      "18920/46375 (epoch 50), train_loss = 1.446, time/batch = 0.095\n",
      ">> sample mode:\n",
      "The signing to French will a curse,\n",
      "And King in the he\n",
      "----------------------------------\n",
      "19291/46375 (epoch 51), train_loss = 1.446, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The joy, I know slay.\n",
      "\n",
      "Second Cyfant:\n",
      "Tolding me wound\n",
      "----------------------------------\n",
      "19662/46375 (epoch 52), train_loss = 1.445, time/batch = 0.054\n",
      ">> sample mode:\n",
      "The hath have he\n",
      "bring wake it, as I give 'Tis drunkan\n",
      "----------------------------------\n",
      "20033/46375 (epoch 53), train_loss = 1.444, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The hurborious Coriolanuing I mads for come offing\n",
      "Hat\n",
      "----------------------------------\n",
      "20404/46375 (epoch 54), train_loss = 1.444, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The not;''? Sight honesty,\n",
      "And thou, by Godfran noble \n",
      "----------------------------------\n",
      "20775/46375 (epoch 55), train_loss = 1.443, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The world!\n",
      "Come as it it gifty that he shall it, my se\n",
      "----------------------------------\n",
      "21146/46375 (epoch 56), train_loss = 1.442, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The was to dishonourt,\n",
      "Shall the sir,\n",
      "Garelloon;\n",
      "I wil\n",
      "----------------------------------\n",
      "21517/46375 (epoch 57), train_loss = 1.442, time/batch = 0.088\n",
      ">> sample mode:\n",
      "The quarrel, I know his crork.\n",
      "\n",
      "BENVOLIO:\n",
      "Blord before\n",
      "----------------------------------\n",
      "21888/46375 (epoch 58), train_loss = 1.441, time/batch = 0.076\n",
      ">> sample mode:\n",
      "The women ruteriald\n",
      "Of justicishion, I can so with\n",
      "see\n",
      "----------------------------------\n",
      "22259/46375 (epoch 59), train_loss = 1.441, time/batch = 0.080\n",
      ">> sample mode:\n",
      "The field up champatitation, it is tell, mot neither o\n",
      "----------------------------------\n",
      "22630/46375 (epoch 60), train_loss = 1.440, time/batch = 0.092\n",
      ">> sample mode:\n",
      "The eyes. The crown-cup intent,\n",
      "As I behold you do ter\n",
      "----------------------------------\n",
      "23001/46375 (epoch 61), train_loss = 1.439, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The her leath amazed,\n",
      "For can I deny hawn brother of t\n",
      "----------------------------------\n",
      "23372/46375 (epoch 62), train_loss = 1.439, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The kindly, lethought of what warrant will sleepy.\n",
      "\n",
      "SI\n",
      "----------------------------------\n",
      "23743/46375 (epoch 63), train_loss = 1.438, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The such accident with her bitter, boy,\n",
      "To pursue: and\n",
      "----------------------------------\n",
      "24114/46375 (epoch 64), train_loss = 1.438, time/batch = 0.087\n",
      ">> sample mode:\n",
      "The makes,\n",
      "Is no tradion.\n",
      "\n",
      "LADY CAPULET:\n",
      "Well, who bar\n",
      "----------------------------------\n",
      "24485/46375 (epoch 65), train_loss = 1.437, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The ware,\n",
      "Which I can of this heads and yours ones far\n",
      "----------------------------------\n",
      "24856/46375 (epoch 66), train_loss = 1.437, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The him:-for you prison, of the privers, though:\n",
      "Is up\n",
      "----------------------------------\n",
      "25227/46375 (epoch 67), train_loss = 1.437, time/batch = 0.085\n",
      ">> sample mode:\n",
      "The shrug: the way words root.\n",
      "\n",
      "SICINIUS:\n",
      "Do thou way?\n",
      "----------------------------------\n",
      "25598/46375 (epoch 68), train_loss = 1.436, time/batch = 0.058\n",
      ">> sample mode:\n",
      "The earth, nor seek, Lark\n",
      "Farry by.\n",
      "\n",
      "TYRRKE SITAUNT:\n",
      "S\n",
      "----------------------------------\n",
      "25969/46375 (epoch 69), train_loss = 1.436, time/batch = 0.219\n",
      ">> sample mode:\n",
      "The thinkeans my brother'd.\n",
      "\n",
      "CATESBY:\n",
      "\n",
      "NORFOLK:\n",
      "Why, a\n",
      "----------------------------------\n",
      "26340/46375 (epoch 70), train_loss = 1.435, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The her;\n",
      "And then horsed lead you blasted, o'erchles o\n",
      "----------------------------------\n",
      "26711/46375 (epoch 71), train_loss = 1.435, time/batch = 0.079\n",
      ">> sample mode:\n",
      "The horse\n",
      "nothings.\n",
      "Hark no love; what slaugh's your f\n",
      "----------------------------------\n",
      "27082/46375 (epoch 72), train_loss = 1.434, time/batch = 0.071\n",
      ">> sample mode:\n",
      "The emptels he shall not just's heaft be nursed\n",
      "Than\n",
      "T\n",
      "----------------------------------\n",
      "27453/46375 (epoch 73), train_loss = 1.434, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The surpience, borns, Wish.\n",
      "\n",
      "CORIOLANUS:\n",
      "\n",
      "GREMIO:\n",
      "The\n",
      "\n",
      "----------------------------------\n",
      "27824/46375 (epoch 74), train_loss = 1.434, time/batch = 0.069\n",
      ">> sample mode:\n",
      "The woes becomes the king's king's jarties death rumou\n",
      "----------------------------------\n",
      "28195/46375 (epoch 75), train_loss = 1.433, time/batch = 0.064\n",
      ">> sample mode:\n",
      "The bester of Vereast's a monts full rackfes with myst\n",
      "----------------------------------\n",
      "28566/46375 (epoch 76), train_loss = 1.433, time/batch = 0.070\n",
      ">> sample mode:\n",
      "The still doth\n",
      "Take my lances with my coup\n",
      "Again.\n",
      "\n",
      "GRE\n",
      "----------------------------------\n",
      "28937/46375 (epoch 77), train_loss = 1.432, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The hath truth the fiery minds not\n",
      "Adventumely, dear, \n",
      "----------------------------------\n",
      "29308/46375 (epoch 78), train_loss = 1.432, time/batch = 0.070\n",
      ">> sample mode:\n",
      "The young Harride, be more heavy fasafes your words:\n",
      "H\n",
      "----------------------------------\n",
      "29679/46375 (epoch 79), train_loss = 1.432, time/batch = 0.081\n",
      ">> sample mode:\n",
      "The were being one back,\n",
      "\n",
      "Partire with home to any mai\n",
      "----------------------------------\n",
      "30050/46375 (epoch 80), train_loss = 1.432, time/batch = 0.074\n",
      ">> sample mode:\n",
      "The gold and comes just give of,\n",
      "Jusing myself of the \n",
      "----------------------------------\n",
      "30421/46375 (epoch 81), train_loss = 1.431, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The crobin me, my lord! Come my world,\n",
      "And go, enclora\n",
      "----------------------------------\n",
      "30792/46375 (epoch 82), train_loss = 1.431, time/batch = 0.081\n",
      ">> sample mode:\n",
      "The harm that night a mocking! thou not be blued this \n",
      "----------------------------------\n",
      "31163/46375 (epoch 83), train_loss = 1.431, time/batch = 0.056\n",
      ">> sample mode:\n",
      "The dishonour; here to all his it.\n",
      "\n",
      "RATCLIFF:\n",
      "If then \n",
      "----------------------------------\n",
      "31534/46375 (epoch 84), train_loss = 1.430, time/batch = 0.100\n",
      ">> sample mode:\n",
      "The people man, who was wash?\n",
      "Against follow from suid\n",
      "----------------------------------\n",
      "31905/46375 (epoch 85), train_loss = 1.430, time/batch = 0.102\n",
      ">> sample mode:\n",
      "The him is we stall make usteed duty me comeon\n",
      "By desi\n",
      "----------------------------------\n",
      "32276/46375 (epoch 86), train_loss = 1.430, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The with her; and up done;\n",
      "He's a queen to me, for thi\n",
      "----------------------------------\n",
      "32647/46375 (epoch 87), train_loss = 1.430, time/batch = 0.072\n",
      ">> sample mode:\n",
      "The I discover'd upon their body words, that hear.\n",
      "\n",
      "JU\n",
      "----------------------------------\n",
      "33018/46375 (epoch 88), train_loss = 1.429, time/batch = 0.094\n",
      ">> sample mode:\n",
      "The he you bring me\n",
      "I in the man maid\n",
      "Is with Kate: th\n",
      "----------------------------------\n",
      "33389/46375 (epoch 89), train_loss = 1.429, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The eye,\n",
      "And we are draws, and with his way:\n",
      "Beside,\n",
      "A\n",
      "----------------------------------\n",
      "33760/46375 (epoch 90), train_loss = 1.429, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The wrongs, and show must no trody heart, the Crafteen\n",
      "----------------------------------\n",
      "34131/46375 (epoch 91), train_loss = 1.429, time/batch = 0.083\n",
      ">> sample mode:\n",
      "The he, Gremio, is all-heart of his natingst\n",
      "To should\n",
      "----------------------------------\n",
      "34502/46375 (epoch 92), train_loss = 1.429, time/batch = 0.058\n",
      ">> sample mode:\n",
      "The right\n",
      "Upon their thing than good loving mouth? O E\n",
      "----------------------------------\n",
      "34873/46375 (epoch 93), train_loss = 1.428, time/batch = 0.112\n",
      ">> sample mode:\n",
      "The will be\n",
      "From my idshis father's age:\n",
      "It waste thee\n",
      "----------------------------------\n",
      "35244/46375 (epoch 94), train_loss = 1.428, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The womans by him,\n",
      "A coural to you, wrinkly a fault,--\n",
      "----------------------------------\n",
      "35615/46375 (epoch 95), train_loss = 1.428, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The were not show noture was hare you, Antio, fingear,\n",
      "----------------------------------\n",
      "35986/46375 (epoch 96), train_loss = 1.428, time/batch = 0.109\n",
      ">> sample mode:\n",
      "The with me,\n",
      "As any king, to be blessed marry to hear'\n",
      "----------------------------------\n",
      "36357/46375 (epoch 97), train_loss = 1.428, time/batch = 0.074\n",
      ">> sample mode:\n",
      "The are if any tongue!\n",
      "\n",
      "DUKE OF YORK:\n",
      "And live, come\n",
      "M\n",
      "----------------------------------\n",
      "36728/46375 (epoch 98), train_loss = 1.428, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The horse to be one when I can small of which eass'\n",
      "ga\n",
      "----------------------------------\n",
      "37099/46375 (epoch 99), train_loss = 1.428, time/batch = 0.086\n",
      ">> sample mode:\n",
      "The pack. No, I would a procome.\n",
      "The uncle\n",
      "Thou did mo\n",
      "----------------------------------\n",
      "37470/46375 (epoch 100), train_loss = 1.427, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The winders:\n",
      "That some gentleman such to-maginels,\n",
      "And\n",
      "----------------------------------\n",
      "37841/46375 (epoch 101), train_loss = 1.427, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The father\n",
      "Espole, upon them.\n",
      "\n",
      "CORIOLANUS:\n",
      "There in le\n",
      "----------------------------------\n",
      "38212/46375 (epoch 102), train_loss = 1.427, time/batch = 0.129\n",
      ">> sample mode:\n",
      "The his still to your sepet.\n",
      "\n",
      "JULIET:\n",
      "They bold, safet\n",
      "----------------------------------\n",
      "38583/46375 (epoch 103), train_loss = 1.427, time/batch = 0.093\n",
      ">> sample mode:\n",
      "The knibe thyself; but with requite\n",
      "To-marry, indeed, \n",
      "----------------------------------\n",
      "38954/46375 (epoch 104), train_loss = 1.427, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The centresty of esward here to Ilabe child, if you ar\n",
      "----------------------------------\n",
      "39325/46375 (epoch 105), train_loss = 1.427, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The foe.\n",
      "\n",
      "GREMIO:\n",
      "Was can I cannot fond,\n",
      "For is the pr\n",
      "----------------------------------\n",
      "39696/46375 (epoch 106), train_loss = 1.427, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The butcher than his leave you, sir, up he that scary \n",
      "----------------------------------\n",
      "40067/46375 (epoch 107), train_loss = 1.427, time/batch = 0.079\n",
      ">> sample mode:\n",
      "The shorth you to him.\n",
      "\n",
      "LADY ANNI:\n",
      "Then, because and D\n",
      "----------------------------------\n",
      "40438/46375 (epoch 108), train_loss = 1.427, time/batch = 0.111\n",
      ">> sample mode:\n",
      "The shall; that when you sumirest.\n",
      "Let me honest.\n",
      "\n",
      "ESC\n",
      "----------------------------------\n",
      "40809/46375 (epoch 109), train_loss = 1.426, time/batch = 0.066\n",
      ">> sample mode:\n",
      "The conterty here.\n",
      "The enequent her;\n",
      "And I cannot be V\n",
      "----------------------------------\n",
      "41180/46375 (epoch 110), train_loss = 1.426, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The with a purpose,\n",
      "To do you humbly infereners from m\n",
      "----------------------------------\n",
      "41551/46375 (epoch 111), train_loss = 1.426, time/batch = 0.069\n",
      ">> sample mode:\n",
      "The king,\n",
      "That disprovight\n",
      "If you is through of dead!\n",
      "\n",
      "----------------------------------\n",
      "41922/46375 (epoch 112), train_loss = 1.426, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The entreators! thus he be confission give it this hou\n",
      "----------------------------------\n",
      "42293/46375 (epoch 113), train_loss = 1.426, time/batch = 0.093\n",
      ">> sample mode:\n",
      "The herself?\n",
      "You too firded\n",
      "Were welcome fight,\n",
      "O'er o\n",
      "----------------------------------\n",
      "42664/46375 (epoch 114), train_loss = 1.426, time/batch = 0.058\n",
      ">> sample mode:\n",
      "The matter smile, you, and metevellign, now I do;\n",
      "I'll\n",
      "----------------------------------\n",
      "43035/46375 (epoch 115), train_loss = 1.426, time/batch = 0.064\n",
      ">> sample mode:\n",
      "The he had, yet in dine from me us.\n",
      "\n",
      "SICINIUS:\n",
      "Yet wit\n",
      "----------------------------------\n",
      "43406/46375 (epoch 116), train_loss = 1.426, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The his revenged and comparated on such away my provos\n",
      "----------------------------------\n",
      "43777/46375 (epoch 117), train_loss = 1.426, time/batch = 0.054\n",
      ">> sample mode:\n",
      "The counterfard-peast elpmon way's harth.\n",
      "\n",
      "ARCHBISHOP \n",
      "----------------------------------\n",
      "44148/46375 (epoch 118), train_loss = 1.426, time/batch = 0.093\n",
      ">> sample mode:\n",
      "The warriove!\n",
      "\n",
      "BRUTUS:\n",
      "But leave\n",
      "your applubtion thus \n",
      "----------------------------------\n",
      "44519/46375 (epoch 119), train_loss = 1.426, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The was two bese of her.\n",
      "\n",
      "LUCENTIO:\n",
      "No would serd is h\n",
      "----------------------------------\n",
      "44890/46375 (epoch 120), train_loss = 1.426, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The harboury\n",
      "That husband should give him.\n",
      "\n",
      "DUKE OF YO\n",
      "----------------------------------\n",
      "45261/46375 (epoch 121), train_loss = 1.426, time/batch = 0.105\n",
      ">> sample mode:\n",
      "The wever to the good person, Isabel with speecily clo\n",
      "----------------------------------\n",
      "45632/46375 (epoch 122), train_loss = 1.425, time/batch = 0.120\n",
      ">> sample mode:\n",
      "The more weet;\n",
      "Like are songs it:\n",
      "What, call me alone.\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = sess.run(model.initial_state) # (2x[60x128])\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        with tf.variable_scope(\"rnn\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='The ', sampling_type=1))\n",
    "            print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0120EN_DSX)This is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for completing this lesson!\n",
    "Created by: <a href = \"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a></h4>\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
